{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_prep\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Lambda)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import _pickle as pickle\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing paths to audio and text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text file:  0\n",
      "text file:  50\n",
      "text file:  100\n",
      "text file:  150\n",
      "text file:  200\n",
      "text file:  250\n",
      "text file:  300\n",
      "text file:  350\n",
      "text file:  400\n",
      "text file:  450\n",
      "text file:  500\n",
      "text file:  550\n"
     ]
    }
   ],
   "source": [
    "audio_paths, texts = src.data_prep.get_audio_and_text_data('/Users/joshbernd/Desktop/gal_notes/Capstone/Speech_Detection/data/LibriSpeech_train/train-clean-100/')        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joshbernd/Desktop/gal_notes/Capstone/Speech_Detection/data/LibriSpeech_train/train-clean-100/8580/287364/8580-287364-0003.flac'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_paths[111]#, texts[111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's store our phonemes and text for safe keeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"data/ipa.txt\", \"w\") as f:\n",
    "    for t in texts:\n",
    "        f.write(str(t) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"data/audio_path.txt\", \"w\") as f:\n",
    "    for a in audio_paths:\n",
    "        f.write(str(a) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading in saved texts\n",
    "texts = []\n",
    "with open(\"data/ipa.txt\", \"r\") as f:\n",
    "  for line in f:\n",
    "    texts.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list = []\n",
    "with open('data/audio_path.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        audio_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'æ',\n",
       " 'd',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 'ɪ',\n",
       " 'ˌ',\n",
       " 'b',\n",
       " 'i',\n",
       " 'ˈ',\n",
       " 'f',\n",
       " 'ɔ',\n",
       " 'r',\n",
       " 'ə',\n",
       " 'p',\n",
       " 'ɛ',\n",
       " 'v',\n",
       " 't',\n",
       " 'n',\n",
       " 'z',\n",
       " 'a',\n",
       " 'ʊ',\n",
       " 'k',\n",
       " 's',\n",
       " 'j',\n",
       " 'm',\n",
       " 'o',\n",
       " 'ð',\n",
       " 'u',\n",
       " 'w',\n",
       " 'ɑ',\n",
       " 'ŋ',\n",
       " 'g',\n",
       " 'θ',\n",
       " 'ʧ',\n",
       " 'c',\n",
       " '*',\n",
       " 'ʃ',\n",
       " 'ʤ',\n",
       " 'ʒ',\n",
       " 'y',\n",
       " \"'\",\n",
       " 'x',\n",
       " 'q']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all elements in text list:\n",
    "unique = []\n",
    "for t in texts:\n",
    "    for char in t:\n",
    "        if char not in unique:\n",
    "            unique.append(char)\n",
    "            \n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hæd leɪd ˌbiˈfɔr hər ə pɛr əv ɔlˈtərnətɪvz naʊ əv kɔrs jʊr kəmˈplitli jʊr oʊn ˈmɪstrəs ənd ər ɛz fri ɛz ðə bərd ɔn ðə baʊ aɪ doʊnt min ju wər nɑt soʊ ˌbiˈfɔr bət jʊr æt ˈprɛzənt ɔn ə ˈdɪfərənt ˈfʊtɪŋ',\n",
       " '/Users/joshbernd/Desktop/gal_notes/Capstone/Speech_Detection/data/LibriSpeech_train/train-clean-100/1069/133709/1069-133709-0000.flac')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0], audio_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to get the rnn to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 47)          7567      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 47)          13536     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 20)                1360      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 22,673\n",
      "Trainable params: 22,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=161, output_dim=47))\n",
    "\n",
    "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
    "model.add(layers.GRU(47, return_sequences=True))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "model.add(layers.SimpleRNN(20))\n",
    "\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Activation\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Lambda)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn_model(input_dim, output_dim=47):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    print(input_data.shape)\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(output_dim, return_sequences=True, \n",
    "                 implementation=2, name='rnn')(input_data)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(simp_rnn)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 161)\n",
      "Model: \"functional_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, None, 161)]       0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 47)          29610     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 47)          0         \n",
      "=================================================================\n",
      "Total params: 29,610\n",
      "Trainable params: 29,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_0 = simple_rnn_model(input_dim=161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered load_train_data\n",
      "entered load_metadata\n",
      "loaded metadata\n",
      "entered fit_train\n",
      "fitted metadata\n",
      "entered load_validation_data\n",
      "entered load_metadata\n",
      "entered fit_train\n",
      "validation_steps:  1\n",
      "initialized ctc model\n",
      "compiled model\n",
      "made sure results path exists\n",
      "calling model.fit_generator\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-10177bbb0cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model(input_to_softmax=model_0, \n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mpickle_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_0.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0msave_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_0.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m            )\n",
      "\u001b[0;32m<ipython-input-108-aa8d7f18d440>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(input_to_softmax, pickle_path, save_model_path, train_data, validation_data, minibatch_size, optimizer, epochs, verbose)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'calling model.fit_generator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     hist = model.fit(generator=audio_generator.next_train(),\n\u001b[0m\u001b[1;32m     62\u001b[0m                      \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'generator'"
     ]
    }
   ],
   "source": [
    "train_model(input_to_softmax=model_0, \n",
    "            pickle_path='model_0.pickle', \n",
    "            save_model_path='model_0.h5',\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's develop training pipeline and AudioGenerator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_prep import calc_feat_dim, spectrogram_from_file, text_to_int_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def add_ctc_loss(input_to_softmax):\n",
    "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
    "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
    "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "            [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model(\n",
    "    inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
    "    outputs=loss_out)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(input_to_softmax, \n",
    "                pickle_path, \n",
    "                save_model_path, \n",
    "                train_data='data/audio_path.txt',\n",
    "                validation_data=\"data/ipa.txt\", \n",
    "                minibatch_size=20,\n",
    "                optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "                epochs=1,\n",
    "                verbose=1,):\n",
    "    \n",
    "    ## create audio_generator instance to get batches of audio\n",
    "    audio_generator = AudioGenerator(minibatch_size=minibatch_size,)\n",
    "    audio_generator.load_train_data()      # went here\n",
    "    audio_generator.load_validation_data() # went here\n",
    "    # calculate steps_per_epoch\n",
    "    num_train_examples=len(audio_generator.train_audio)\n",
    "    steps_per_epoch = num_train_examples//minibatch_size\n",
    "    # calculate validation_steps\n",
    "    num_valid_samples = len(audio_generator.valid_audio) \n",
    "    validation_steps = num_valid_samples//minibatch_size\n",
    "    print('validation_steps: ', validation_steps)\n",
    "    # add CTC loss to the NN specified in input_to_softmax\n",
    "    model = add_ctc_loss(input_to_softmax)\n",
    "    print('initialized ctc model')\n",
    "    \n",
    "    \n",
    "    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "    print('compiled model')\n",
    "    \n",
    "    # make results/ directory, if necessary\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    print('made sure results path exists')    \n",
    "    # add checkpointer\n",
    "    checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, \n",
    "                                   verbose=0)\n",
    "    \n",
    "    # train model\n",
    "    print('calling model.fit_generator')\n",
    "    hist = model.fit(generator=audio_generator.next_train(),\n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     epochs=epochs,\n",
    "                     validation_data=audio_generator.next_valid(),\n",
    "                     validation_steps=validation_steps,\n",
    "                     callbacks=[checkpointer], \n",
    "                     verbose=verbose)\n",
    "    print('saving model loss')\n",
    "    \n",
    "    # save model loss\n",
    "    with open('results/'+pickle_path, 'wb') as f:\n",
    "        pickle.dump(hist.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make an audio generator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "                        \n",
    "RNG_SEED = 123                        \n",
    "class AudioGenerator():\n",
    "    def __init__(self, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
    "        minibatch_size=20, desc_file=None,):\n",
    "        \n",
    "        \"\"\"\n",
    "        Params:\n",
    "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
    "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
    "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "                [0, max_freq] are returned (for spectrogram ONLY)\n",
    "            desc_file (str, optional): Path to a JSON-line file that contains\n",
    "                labels and paths to the audio files. If this is None, then\n",
    "                load metadata right away\n",
    "        \"\"\"\n",
    "    \n",
    "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
    "        self.feats_mean = np.zeros((self.feat_dim))\n",
    "        self.feats_std = np.ones((self.feat_dim,))\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.max_freq = max_freq\n",
    "        self.rng = random.Random(RNG_SEED)\n",
    "        self.cur_train_index = 0\n",
    "        self.cur_valid_index = 0\n",
    "        self.cur_test_index = 0\n",
    "        self.minibatch_size = minibatch_size\n",
    "    \n",
    "    def get_batch(self, partition):\n",
    "        print('entered get_batch')\n",
    "        if partition == 'train':\n",
    "            audio_paths = self.train_audio\n",
    "            cur_index = self.cur_train_index\n",
    "            texts = self.train_texts\n",
    "        elif partition == 'valid':\n",
    "            audio_paths = self.valid_audio\n",
    "            cur_index = self.cur_valid_index\n",
    "            print('cur_index: ', cur_index)\n",
    "            texts = self.valid_texts\n",
    "#         elif partition == 'test':\n",
    "#             audio_paths = self.test_audio_paths\n",
    "#             cur_index = self.test_valid_index\n",
    "#             texts = self.test_texts\n",
    "            \n",
    "        features = [self.normalize(self.featurize(a)) for a in audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
    "\n",
    "#         calculate necessary sizes\n",
    "        max_length = max([features[i].shape[0] for i in range(0, self.minibatch_size)])\n",
    "        max_string_length = max([len(texts[cur_index+i]) for i in range(0, self.minibatch_size)])     \n",
    "        \n",
    "        \n",
    "        X_data = np.zeros([self.minibatch_size, max_length, self.feat_dim])\n",
    "        labels = np.ones([self.minibatch_size, max_string_length]) * 47\n",
    "        \n",
    "        \n",
    "#         X_data = np.zeros([self.minibatch_size, self.feat_dim])\n",
    "#         labels = np.ones([self.minibatch_size]) * 47\n",
    "                \n",
    "        \n",
    "        input_length = np.zeros([self.minibatch_size, 1])\n",
    "        label_length = np.zeros([self.minibatch_size, 1])\n",
    "        \n",
    "        for i in range(0, self.minibatch_size):\n",
    "            # get each featurized/normalized spectrogram\n",
    "            feat = features[i]\n",
    "            input_length[i] = feat.shape[0]\n",
    "            X_data[i, :feat.shape[0], :] = feat\n",
    "            \n",
    "            #calc labels and lable length\n",
    "            label = np.array(text_to_int_sequence(texts[cur_index+i]))\n",
    "            labels[i, :len(label)] = label\n",
    "            label_length[i] = len(label)\n",
    "        print('shape of X_data: ', X_data.shape)   \n",
    "        print('shape of labels: ', labels.shape)\n",
    "        #return the arrays\n",
    "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
    "        inputs = {'the_input': X_data,\n",
    "                 'the labels': labels, \n",
    "                 'input_length': input_length, \n",
    "                 'label_length': label_length\n",
    "                 }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    \n",
    "    def shuffle_data_by_partition(self, partition):\n",
    "        print('entered shuffle_data_by_partition')\n",
    "        \"\"\" Shuffle the training or validation data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio, self.train_texts = shuffle_data(\n",
    "                self.train_audio, self.train_texts)\n",
    "#         elif partition == 'valid':\n",
    "#             self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
    "#                 self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "#         else:\n",
    "#             raise Exception(\"Invalid partition. \"\n",
    "#                 \"Must be train/validation\")\n",
    "        \n",
    "  \n",
    "    def fit_train(self, k_samples=100):\n",
    "        print('entered fit_train')\n",
    "        \"\"\" Estimate the mean and std of the features from the training set\n",
    "        Params:\n",
    "            k_samples (int): Use this number of samples for estimation\n",
    "        \"\"\"\n",
    "        k_samples = min(k_samples, len(self.train_audio))\n",
    "        samples = self.rng.sample(self.train_audio, k_samples)\n",
    "        feats = [self.featurize(s) for s in samples]\n",
    "        feats = np.vstack(feats)\n",
    "        self.feats_mean = np.mean(feats, axis=0)\n",
    "        self.feats_std = np.std(feats, axis=0)\n",
    "\n",
    "\n",
    "    def featurize(self, audio_clip):\n",
    "#         print('entered featurize')\n",
    "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
    "        Params:\n",
    "            audio_clip (str): Path to the audio clip\n",
    "        \"\"\"\n",
    "        \n",
    "        return spectrogram_from_file(audio_clip, \n",
    "                                     step=self.step, \n",
    "                                     window=self.window,\n",
    "                                     max_freq=self.max_freq)\n",
    "\n",
    "\n",
    "    def normalize(self, feature, eps=1e-14):\n",
    "#         print('entered normalize')\n",
    "        \"\"\" Center a feature using the mean and std\n",
    "        Params:\n",
    "            feature (numpy.ndarray): Feature to normalize\n",
    "        \"\"\"\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        print('entered load_train_data')\n",
    "        self.load_metadata('train')\n",
    "        print('loaded metadata')\n",
    "        self.fit_train()\n",
    "        print('fitted metadata')\n",
    "\n",
    "    def load_validation_data(self):\n",
    "        print('entered load_validation_data')\n",
    "        self.load_metadata('validation')\n",
    "        self.fit_train()\n",
    "                \n",
    "                    \n",
    "    def load_metadata(self, partition):\n",
    "        print('entered load_metadata')\n",
    "        audio_list = []\n",
    "        texts = []\n",
    "\n",
    "        with open('data/audio_path.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                audio_list.append(line.strip())\n",
    "        with open(\"data/ipa.txt\", \"r\") as f:\n",
    "          for line in f:\n",
    "            texts.append(line.strip())\n",
    "            \n",
    "        ## testing on 100 samples to make sure it works\n",
    "        audio_list = audio_list[:100]\n",
    "        texts = texts[:100]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(audio_list, \n",
    "                                                          texts, \n",
    "                                                          test_size=0.33, \n",
    "                                                          random_state=42)        \n",
    "        if partition == 'train':\n",
    "            self.train_audio = X_train\n",
    "            self.train_texts = y_train\n",
    "        elif partition == 'validation':\n",
    "            self.valid_audio = X_val\n",
    "            self.valid_texts = y_val\n",
    "        \n",
    "        \n",
    "    def next_train(self):\n",
    "        '''\n",
    "        get next batch of training data\n",
    "        '''\n",
    "        print('entered next_train')\n",
    "        i = 0\n",
    "        while True:\n",
    "            print(i)\n",
    "            ret = self.get_batch('train')\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
    "                self.cur_train_index = 0\n",
    "                self.shuffle_data_by_partition('train')\n",
    "            i += 1\n",
    "            yield ret\n",
    "        \n",
    "        \n",
    "    def next_valid(self):\n",
    "        '''\n",
    "        get next batch of validation data\n",
    "        '''\n",
    "        print('entered next_valid')\n",
    "        while True:\n",
    "            ret = self.get_batch('valid')\n",
    "            self.cur_valid_index += self.minibatch_size\n",
    "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
    "                self.cur_valid_index = 0\n",
    "                self.shuffle_data_by_partition('valid')\n",
    "            yield ret\n",
    "        \n",
    "        \n",
    "def shuffle_data(audio_paths, texts):\n",
    "    print('entered shuffle_data')\n",
    "    \"\"\" Shuffle the data (called after making a complete pass through \n",
    "        training or validation data during the training process)\n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.random.permutation(len(audio_paths))\n",
    "    audio_paths = [audio_paths[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiogen = AudioGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cur_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-76254d16b24a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m47\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_int_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cur_index' is not defined"
     ]
    }
   ],
   "source": [
    "np.ones([20]) * 47\n",
    "label = np.array(text_to_int_sequence(texts[cur_index+i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hæd leɪd ˌbiˈfɔr hər ə pɛr əv ɔlˈtərnətɪvz naʊ əv kɔrs jʊr kəmˈplitli jʊr oʊn ˈmɪstrəs ənd ər ɛz fri ɛz ðə bərd ɔn ðə baʊ aɪ doʊnt min ju wər nɑt soʊ ˌbiˈfɔr bət jʊr æt ˈprɛzənt ɔn ə ˈdɪfərənt ˈfʊtɪŋ'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map_str = \"\"\"\n",
    "\n",
    "' 0\n",
    "<SPACE> 1\n",
    "'h' 2 \n",
    "'æ' 3 \n",
    "'d' 4 \n",
    "'q' 5 \n",
    "'l' 6 \n",
    "'e' 7 \n",
    "'ɪ' 8 \n",
    "'ˌ' 9 \n",
    "'b' 10 \n",
    "'i' 11 \n",
    "'ˈ' 12 \n",
    "'f' 13 \n",
    "'ɔ' 14 \n",
    "'r' 15 \n",
    "'ə' 16 \n",
    "'p' 17 \n",
    "'ɛ' 18 \n",
    "'v' 19 \n",
    "'t' 20  \n",
    "'n' 21\n",
    "'z' 22\n",
    "'a' 23\n",
    "'ʊ' 24\n",
    "'k' 25\n",
    "'s' 26\n",
    "'j' 27\n",
    "'m' 28\n",
    "'o' 29 \n",
    "'ð' 30 \n",
    "'u' 31 \n",
    "'w' 32 \n",
    "'ɑ' 33 \n",
    "'ŋ' 34 \n",
    "'g' 35 \n",
    "'θ' 36 \n",
    "'ʧ' 37 \n",
    "'c' 38 \n",
    "'*' 39 \n",
    "'ʃ' 40 \n",
    "'ʤ' 41 \n",
    "'ʒ' 42 \n",
    "'y' 43 \n",
    "\"'\" 44 \n",
    "'x' 45 \n",
    " \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "for line in char_map_str.strip().split('\\n'):\n",
    "    ch, index = line.split()\n",
    "    char_map[ch] = int(index)\n",
    "    index_map[int(index)+1] = ch\n",
    "index_map[2] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\": 0,\n",
       " '<SPACE>': 1,\n",
       " \"'h'\": 2,\n",
       " \"'æ'\": 3,\n",
       " \"'d'\": 4,\n",
       " \"'q'\": 5,\n",
       " \"'l'\": 6,\n",
       " \"'e'\": 7,\n",
       " \"'ɪ'\": 8,\n",
       " \"'ˌ'\": 9,\n",
       " \"'b'\": 10,\n",
       " \"'i'\": 11,\n",
       " \"'ˈ'\": 12,\n",
       " \"'f'\": 13,\n",
       " \"'ɔ'\": 14,\n",
       " \"'r'\": 15,\n",
       " \"'ə'\": 16,\n",
       " \"'p'\": 17,\n",
       " \"'ɛ'\": 18,\n",
       " \"'v'\": 19,\n",
       " \"'t'\": 20,\n",
       " \"'n'\": 21,\n",
       " \"'z'\": 22,\n",
       " \"'a'\": 23,\n",
       " \"'ʊ'\": 24,\n",
       " \"'k'\": 25,\n",
       " \"'s'\": 26,\n",
       " \"'j'\": 27,\n",
       " \"'m'\": 28,\n",
       " \"'o'\": 29,\n",
       " \"'ð'\": 30,\n",
       " \"'u'\": 31,\n",
       " \"'w'\": 32,\n",
       " \"'ɑ'\": 33,\n",
       " \"'ŋ'\": 34,\n",
       " \"'g'\": 35,\n",
       " \"'θ'\": 36,\n",
       " \"'ʧ'\": 37,\n",
       " \"'c'\": 38,\n",
       " \"'*'\": 39,\n",
       " \"'ʃ'\": 40,\n",
       " \"'ʤ'\": 41,\n",
       " \"'ʒ'\": 42,\n",
       " \"'y'\": 43,\n",
       " '\"\\'\"': 44,\n",
       " \"'x'\": 45}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
